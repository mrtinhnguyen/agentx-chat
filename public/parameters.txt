Parameters

streamText() 
temperature
    Temperature setting. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.
topP
    Nucleus sampling. The value is passed through to the provider. The range depends on the provider and model. It is recommended to set either `temperature` or `topP`, but not both.
topK
    Only sample from the top K options for each subsequent token. Used to remove "long tail" low probability responses. Recommended for advanced use cases only. You usually only need to use temperature.
seed
    The seed (integer) to use for random sampling. If set and supported by the model, calls will generate deterministic results.
maxRetries
    Maximum number of retries. Set to 0 to disable retries. Default: 2.
presencePenalty
    Presence penalty setting. It affects the likelihood of the model to repeat information that is already in the prompt. The value is passed through to the provider. The range depends on the provider and model.
frequencyPenalty
    Frequency penalty setting. It affects the likelihood of the model to repeatedly use the same words or phrases. The value is passed through to the provider. The range depends on the provider and model.
maxOutputTokens
    Maximum number of tokens to generate.
toolChoice
    "auto" | "none" | "required" | { "type": "tool", "toolName": string }
    The tool choice setting. It specifies how tools are selected for execution. The default is "auto". "none" disables tool execution. "required" requires tools to be executed. { "type": "tool", "toolName": string } specifies a specific tool to execute.

openai (Provider)